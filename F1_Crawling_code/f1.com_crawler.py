import asyncio
from playwright.async_api import async_playwright, TimeoutError
from bs4 import BeautifulSoup
import os
import time

f1_teams = {
    "Scuderia_Ferrari": "https://www.formula1.com/en/information/ferrari-year-by-year.61yfcjhl05vSlmNJB1SIJ0",
    "Red_Bull_Racing": "https://www.formula1.com/en/information/red-bull-racing-year-by-year.5gsBMoMf3DhOSBOJ8Cx8Bi",
    "McLaren": "https://www.formula1.com/en/information/mclaren-year-by-year.6Gj22qyOorq5dpniarY3rP",
    "Haas_F1_Team": "https://www.formula1.com/en/information/haas-year-by-year.7DczM4FtRLOOlbMMrMVSaE",
    "Sauber_Motorsport": "https://www.formula1.com/en/information/kick-sauber-year-by-year.JoWXFc6oEcNk5ozeiPxG5",
    "Aston_Martin_in_Formula_One": "https://www.formula1.com/en/information/aston-martin-year-by-year.69C4UPk1FrpRIzE7L4Py9n",
    "Mercedes-Benz_in_Formula_One": "https://www.formula1.com/en/information/mercedes-year-by-year.45gq1OShE3U1H5iEJSVtNd",
    "Williams_Racing": "https://www.formula1.com/en/information/williams-year-by-year.6wHlJglT3USpmIbETtAYzW",
    "Racing_Bulls": "https://www.formula1.com/en/information/rb-year-by-year.RsVCsWpMnPzUr7nNVSlyO",
    "Alpine_F1_Team": "https://www.formula1.com/en/information/alpine-year-by-year.26lcAj4zKxSs1w959B6yV"
}

html_content = None # ì„±ê³µí•œ HTMLì„ ë‹´ì„ ë³€ìˆ˜
# async ì‚¬ìš©
async def crawl_and_save_text(team_key, url):
    source = "f1.com"
    file_name = f"{team_key}_{source}_data.txt"
    output_dir = f"(ENG)F1_{source}"
    os.makedirs(output_dir, exist_ok=True)
    
    try:
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            page = await browser.new_page()
            try:
                print(f"[ {source} ]ì—ì„œ [ {team_key} ] ë°ì´í„° í¬ë¡¤ë§")
                
                response = await page.goto(url, timeout=30000)
                if response is None or response.status >= 400:
                    raise Exception(f"âŒ HTTP ìš”ì²­ ì‹¤íŒ¨: {response.status if response else 'N/A'}")

                # ì¿ í‚¤ íŒì—… ì²˜ë¦¬=======================================================
                try:
                    # ì¿ í‚¤ íŒì—…ì´ ë“¤ì–´ìˆëŠ” iframe ì°¾ê¸°
                    iframe = page.frame_locator("iframe[id*='sp_message_iframe']")
                    # ì¿ í‚¤ ë™ì˜ ë²„íŠ¼ í´ë¦­
                    await iframe.get_by_title("Accept all").click(timeout=5000)
                    print(f"ğŸ‘ [{team_key}] ì¿ í‚¤ ë™ì˜ íŒì—… ì²˜ë¦¬ ì„±ê³µ")
                    await asyncio.sleep(2)

                except Exception as e:
                    print(f"âŒ [{team_key}] ì¿ í‚¤ ë™ì˜ íŒì—… ì²˜ë¦¬ ì‹¤íŒ¨: {str(e)}")
                    pass
                # ====================================================================
                    
                # ë©”ì¸ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
                await page.wait_for_selector('#maincontent', state='attached', timeout=60000) 
                
                html_content = await page.content()

                print(f"ğŸ‘ [{team_key}] í˜ì´ì§€ ë¡œë”© ì„±ê³µ")
                
            except (TimeoutError, Exception) as e:                    
                if attempt == MAX_RETRIES - 1:
                    print(f"âŒ [{team_key}] í˜ì´ì§€ ë¡œë”© ìµœì¢… ì‹¤íŒ¨: {str(e)}")
                    await browser.close()
                    return None
                
                print(f"âŒ [{team_key}] í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {str(e)} (ë‚¨ì€ ì‹œë„ íšŸìˆ˜: {MAX_RETRIES - attempt - 1}íšŒ)")
                await asyncio.sleep(5)
                continue
                
            if not html_content:
                print(f"âŒ [{team_key}] HTML ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None

            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ê°€ì¥ ì•ˆì •ì ì¸ ë¶€ëª¨ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            full_container = soup.select('div[class*="Container-module_inner"] div[class*="content-rich-text"]')
            # print(full_container)

            if not full_container:
                print(f"âŒ [{team_key}] ìµœì¢… ë¶€ëª¨ ì»¨í…Œì´ë„ˆë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
            extracted_text = []
            
            for container in full_container:
                elements = container.select('h3, p')

                for element in elements:
                    # print(element)
                    # ë…¸ì´ì¦ˆ(í‘œ, ê°ì£¼, ì´ë¯¸ì§€, ë¯¸ë””ì–´) ì œê±° ë° í…ìŠ¤íŠ¸ ì œê±°
                    for tag in element.find_all(['a', 'img', 'table']): tag.decompose()

                    text = element.get_text(strip=True)
                    # print(text)
                    if text:
                        if element.name in ['h2', 'h3']:
                            extracted_text.append(f"\n--- {element.name.upper()}: {text} ---")
                        else:
                            extracted_text.append(text)                    
                # print(extracted_text)

            # ê²°ê³¼ë¥¼ TXT íŒŒì¼ë¡œ ì €ì¥
            full_path = os.path.join(output_dir, file_name)

            with open(full_path, 'w', encoding='utf-8') as f:
                f.write(f"URL: {url}\n\n")
                f.write(f"íŒ€ ì´ë¦„: {team_key} \n")
                f.write(f"========== TEAM NARRATIVE DATA ({source}) ==========\n")
                f.write('\n'.join(extracted_text))
            
            print(f"âœ… [{team_key}] ë°ì´í„° í¬ë¡¤ë§ ë° ì €ì¥ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤: {full_path}")
            return full_path

    except Exception as e:
        # íƒ€ì„ì•„ì›ƒ ì˜¤ë¥˜ ì‹œ ë¸Œë¼ìš°ì €ê°€ ë‹«íˆì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì•ˆì „í•˜ê²Œ ë‹«ê¸° ì‹œë„
        try:
            await browser.close() 
        except:
            pass
        print(f"âŒ [{team_key}] í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
        return None

# --- ì‹¤í–‰ ---
# PlaywrightëŠ” ë¹„ë™ê¸° í™˜ê²½ì—ì„œ ì‹¤í–‰í•´ì•¼ í•¨
async def main_async():
    print("í¬ë¡¤ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤...\n")

    tasks = []
    for team_key, url in f1_teams.items():
        # ê° íŒ€ì— ëŒ€í•œ í¬ë¡¤ë§ ì‘ì—…ì„ tasks ë¦¬ìŠ¤íŠ¸ì— ë‹´ëŠ”ë‹¤.
        tasks.append(crawl_and_save_text(team_key, url))
    
    # í•µì‹¬: asyncio.gatherë¥¼ awaitë¡œ ì‹¤í–‰!
    # ì´ ë¶€ë¶„ì´ ë¹„ë™ê¸°(async) í•¨ìˆ˜ ë‚´ë¶€ì—ì„œ ì‹¤í–‰ë˜ì–´ì•¼ í•œë‹¤.
    await asyncio.gather(*tasks) 
    
    print("\nëª¨ë“  íŒ€ì— ëŒ€í•œ í¬ë¡¤ë§ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    # asyncio.run()ì€ ìµœìƒìœ„ ë¹„ë™ê¸° í•¨ìˆ˜(main_async)ë§Œ ì‹¤í–‰í•œë‹¤.
    asyncio.run(main_async())