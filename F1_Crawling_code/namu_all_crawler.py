import asyncio
from playwright.async_api import async_playwright
from bs4 import BeautifulSoup
import os
import re
from urllib.parse import unquote

# --- ì„¤ì • ë° ë°ì´í„° ---

# í¬ë¡¤ë§í•  ëŒ€ìƒ URL ë¦¬ìŠ¤íŠ¸ (ì—¬ê¸°ì— ì›í•˜ëŠ” ë§í¬ë“¤ì„ ì¶”ê°€í•˜ì„¸ìš”)
TARGET_URLS = [
    "https://namu.wiki/w/%ED%8F%AC%EB%AE%AC%EB%9F%AC%201/2025%EC%8B%9C%EC%A6%8C",
    "https://namu.wiki/w/%ED%8F%AC%EB%AE%AC%EB%9F%AC%201/2024%EC%8B%9C%EC%A6%8C",
    "https://namu.wiki/w/%ED%8F%AC%EB%AE%AC%EB%9F%AC%201/2023%EC%8B%9C%EC%A6%8C",
    "https://namu.wiki/w/%ED%8F%AC%EB%AE%AC%EB%9F%AC%201/2022%EC%8B%9C%EC%A6%8C",
    "https://namu.wiki/w/%ED%8F%AC%EB%AE%AC%EB%9F%AC%201/2021%EC%8B%9C%EC%A6%8C",
    "https://namu.wiki/w/%ED%8F%AC%EB%AE%AC%EB%9F%AC%201/2020%EC%8B%9C%EC%A6%8C"
]

# íŒ€ë³„ í‚¤ì›Œë“œ ì •ì˜ (í•œêµ­ì–´ ë° ì˜ì–´)
TEAM_KEYWORDS = {
    "Scuderia_Ferrari": ["í˜ë¼ë¦¬", "Ferrari", "SF-24"],
    "Red_Bull_Racing": ["ë ˆë“œë¶ˆ", "Red Bull", "RB20"],
    "McLaren": ["ë§¥ë¼ë Œ", "McLaren", "MCL38"],
    "Alpine_F1_Team": ["ì•Œí•€", "Alpine", "A524", "ë¥´ë…¸", "Renault", "RNR26"],
    "Haas_F1_Team": ["í•˜ìŠ¤", "Haas", "VF-24"],
    "Sauber_Motorsport": ["ììš°ë²„", "Sauber", "Kick Sauber", "C44", "ì•ŒíŒŒ ë¡œë©”ì˜¤", "Alfa Romeo", "Stake"],
    "Aston_Martin_in_Formula_One": ["ì• ìŠ¤í„´ ë§ˆí‹´", "ì• ìŠ¤í„´ë§ˆí‹´", "Aston Martin", "ë ˆì´ì‹± í¬ì¸íŠ¸", "AMR24"],
    "Mercedes-Benz_in_Formula_One": ["ë©”ë¥´ì„¸ë°ìŠ¤", "ë²¤ì¸ ", "Mercedes", "W15"],
    "Williams_Racing": ["ìœŒë¦¬ì—„ìŠ¤", "Williams", "FW46"],
    "Racing_Bulls": ["ë ˆì´ì‹± ë¶ˆìŠ¤", "Racing Bulls", "VCARB", "RB", "ì•ŒíŒŒ íƒ€ìš°ë¦¬", "Alpha Tauri", "ATR26"]
}

OUTPUT_DIR = "(KOR)F1_namuwiki_season"


async def crawl_namuwiki_content(url):
    """
    ì£¼ì–´ì§„ URLì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤.
    ì œëª©, í‘œ, ì´ë¯¸ì§€, ë™ì˜ìƒ, ë§í¬ í…ìŠ¤íŠ¸ ë“±ì„ ì œì™¸í•©ë‹ˆë‹¤.
    """
    print(f"ğŸ”„ í¬ë¡¤ë§ ì‹œì‘: {url}")
    extracted_data = []

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        # ë´‡ íƒì§€ íšŒí”¼ë¥¼ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ì„¤ì •
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36"
        )
        page = await context.new_page()
        
        try:
            response = await page.goto(url, timeout=60000)
            if response is None or response.status >= 400:
                print(f"âŒ HTTP ìš”ì²­ ì‹¤íŒ¨: {response.status if response else 'N/A'} - {url}")
                await browser.close()
                return None
            
            # ë³¸ë¬¸ ë¡œë”© ëŒ€ê¸° (ê°œìš” ë“± ì£¼ìš” í—¤ë”ê°€ ëœ° ë•Œê¹Œì§€)
            try:
                await page.wait_for_selector('h2', state='attached', timeout=30000)
            except Exception:
                print(f"âš ï¸ H2 íƒœê·¸ë¥¼ ì°¾ëŠ”ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê±°ë‚˜ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤.")

            html_content = await page.content()
            await browser.close() 
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            # ë‚˜ë¬´ìœ„í‚¤ í´ë˜ìŠ¤ëª…ì€ ìì£¼ ë°”ë€Œë¯€ë¡œ, ë¬¸ì„œ ì œëª©(h1)ì„ ì°¾ê³  ê·¸ ë¶€ëª¨ í˜¹ì€ í˜•ì œ ë…¸ë“œë¥¼ íƒìƒ‰í•˜ëŠ” ê²ƒì´ ì•ˆì „í•  ìˆ˜ ìˆìœ¼ë‚˜,
            # í˜„ì¬ ì•Œë ¤ì§„ main container í´ë˜ìŠ¤ë¥¼ ë¨¼ì € ì‹œë„í•˜ê³ , ì—†ìœ¼ë©´ article íƒœê·¸ ë“±ì„ ì°¾ìŠµë‹ˆë‹¤.
            content = soup.find('div', class_='NMmqIPVM _61W7Avfw')
            
            if not content:
                # ëŒ€ì²´ íƒìƒ‰: article íƒœê·¸ ì‹œë„
                content = soup.find('article')
            
            if not content:
                print(f"âŒ ë¬¸ì„œì˜ ë©”ì¸ ì½˜í…ì¸  ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {url}")
                return None
            
            # --- ì œì™¸ ì‘ì—… (Decompose) ---
            # --- ì œì™¸ ì‘ì—… (Decompose) ---
            # 1. ëª©ì°¨(TOC), í‘œ(table), ì´ë¯¸ì§€(img, figure, video), ê°ì£¼(sup, span.wiki-fn-content) ì œê±°
            # nav, aside ë“± ë„ ì œê±°
            for tag in content.find_all(['table', 'img', 'video', 'figure', 'iframe', 'canvas', 'nav', 'aside', 'div', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6']):
                if not tag.name:
                    continue
                    
                # divì˜ ê²½ìš° íŠ¹ì • í´ë˜ìŠ¤(ëª©ì°¨ ë“±)ë§Œ ì œê±°
                if tag.name == 'div':
                    classes = tag.get('class', [])
                    if classes and ('wiki-macro-toc' in classes or 'toc' in classes):
                        tag.decompose()
                # í—¤ë” íƒœê·¸ëŠ” ì œëª©ì´ë¯€ë¡œ ì œì™¸ (ì‚¬ìš©ì ìš”ì²­: ì œëª© ì œì™¸)
                elif tag.name.startswith('h'):
                    tag.decompose()
                else: 
                    tag.decompose()
            
            # ê°ì£¼ ë° í¸ì§‘ ë²„íŠ¼ ì œê±°
            for tag in content.find_all(class_=['wiki-fn-content', 'wiki-edit-date', 'wiki-category']):
                tag.decompose()
            for tag in content.find_all('a', text=re.compile(r'\[í¸ì§‘\]')):
                tag.decompose()

            # --- í…ìŠ¤íŠ¸ ì¶”ì¶œ ---
            # Namuwiki ë³¸ë¬¸ í…ìŠ¤íŠ¸ëŠ” ì£¼ë¡œ class='IBdgNaCn' ì¸ div ë˜ëŠ” li íƒœê·¸ì— ìˆìŒ.
            # ì¤‘ë³µ ë°©ì§€ë¥¼ ìœ„í•´ specific selector ì‚¬ìš©.
            paragraphs = content.select('div.IBdgNaCn, li')
            
            seen_texts = set()

            for p in paragraphs:
                # ë§í¬(a íƒœê·¸)ëŠ” unwrapí•˜ì—¬ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¹€
                for a in p.find_all('a'):
                    a.unwrap()
                
                text = p.get_text(strip=True)
                
                # ì •ì œ ë° ì¤‘ë³µ í•„í„°ë§
                if not text:
                    continue
                if len(text) < 10: 
                    continue
                if text in seen_texts:
                    continue
                
                # ìƒìœ„ ë¬¸ë‹¨ì´ ì¡íˆê³  í•˜ìœ„ ë¬¸ë‹¨ì´ ë˜ ì¡íˆëŠ” ê²½ìš° ë°©ì§€ (í¬í•¨ ê´€ê³„ í™•ì¸ ë“±ì€ ë³µì¡í•˜ë¯€ë¡œ í…ìŠ¤íŠ¸ ì¤‘ë³µìœ¼ë¡œ 1ì°¨ ë°©ì–´)
                # ë§Œì•½ "A B"ê°€ ìˆê³  "A", "B"ê°€ ë”°ë¡œ ì¡íˆë©´?
                # IBdgNaCn í´ë˜ìŠ¤ëŠ” ë³´í†µ ë§ë‹¨ ë¬¸ë‹¨ì— ë¶™ìœ¼ë¯€ë¡œ ì¤‘ì²©ì´ ì ìŒ.
                
                seen_texts.add(text)
                extracted_data.append(text)

        except Exception as e:
            print(f"âŒ í¬ë¡¤ë§ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e} - {url}")
            return None

    return extracted_data


def classify_and_save(all_text_data):
    """
    ìˆ˜ì§‘ëœ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ë¥¼ ìˆœíšŒí•˜ë©° íŒ€ í‚¤ì›Œë“œì— ë”°ë¼ ë¶„ë¥˜í•˜ê³  íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    # íŒ€ë³„ë¡œ ì €ì¥í•  í…ìŠ¤íŠ¸ ë²„í¼
    team_buffers = {key: [] for key in TEAM_KEYWORDS.keys()}
    
    # ë””ë ‰í† ë¦¬ ìƒì„±
    os.makedirs(OUTPUT_DIR, exist_ok=True)
    
    # ìˆœì„œ ë³´ì¥ì„ ìœ„í•´ List ì‚¬ìš©. ì¤‘ë³µ ì œê±° ì—¬ë¶€ëŠ”? 
    # í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ ì²­í¬ê°€ ì—¬ëŸ¬ íŒ€ì— ì†í•  ìˆ˜ ìˆìŒ -> OK (User requirement: "ê° íŒ€ë³„ txtì— ë„£ì–´ì¤˜ì•¼ í•œë‹¤")
    # í•˜ì§€ë§Œ í…ìŠ¤íŠ¸ ì²­í¬ ìì²´ì˜ ì¤‘ë³µ(í¬ë¡¤ë§ ë‹¨ê³„ì—ì„œ ë°œìƒí•œ)ì€ ì œê±°í•´ì•¼ í•¨ (ìœ„ì—ì„œ ì²˜ë¦¬í•¨).
    
    for text in all_text_data:
        matched_teams = set()
        
        # í‚¤ì›Œë“œ ë§¤ì¹­
        for team_key, keywords in TEAM_KEYWORDS.items():
            for kw in keywords:
                if kw in text:
                    matched_teams.add(team_key)
                    # "ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ íŒ€ì„ ì°¾ì§€ ë§ê³ ... ê°ê°ì˜ íŒ€ë³„ txtì— ë„£ì–´ì¤˜ì•¼"
                    # -> break í•˜ì§€ ì•Šê³  ê³„ì† ì°¾ì•„ì„œ multi-labeling?
                    # User: "í•œ ë§í¬ì˜ ë‚´ìš©ì„ ë¬´ì¡°ê±´ í•œ íŒ€ì˜ íŒŒì¼ì— ë„£ì–´ì•¼ í•˜ëŠ” ê±´ ì•„ë‹ˆì•¼... ê°ê°ì˜ íŒ€ë³„ txtì— ë„£ì–´ì¤˜ì•¼ í•œë‹¤ëŠ” ê±¸ ëª…ì‹¬"
                    # This implies multi-classification is required if multiple keywords appear.
                    # My previous code did `break` inside the inner loop (keyword loop) but NOT the outer loop (team loop).
                    # `break` breaks `for kw in keywords`. It proceeds to next `team_key`.
                    # So it WAS multi-labeling correctly.
                    break 
        
        # ë§¤ì¹­ëœ ëª¨ë“  íŒ€ì— í…ìŠ¤íŠ¸ ì¶”ê°€
        if matched_teams:
            for team in matched_teams:
                # ê°„ë‹¨í•œ ì¤‘ë³µ ë°©ì§€ (ë™ì¼ íŒŒì¼ ë‚´ ë™ì¼ í…ìŠ¤íŠ¸)
                if text not in team_buffers[team]:
                    team_buffers[team].append(text)

        else:
            # (ì„ íƒì‚¬í•­) ì–´ë–¤ íŒ€ì—ë„ ì†í•˜ì§€ ì•ŠëŠ” í…ìŠ¤íŠ¸ëŠ” ë²„ë¦¬ê±°ë‚˜ ë³„ë„ ë¡œê·¸?
            # í˜„ì¬ ìš”êµ¬ì‚¬í•­: "ê° íŒ€ì— ê´€í•œ ë‚´ìš©ë§Œ íŒ€ ë³„ ìµœì¢… ê²°ê³¼ë¬¼ txtíŒŒì¼ì— ë„£ì–´ì¤˜" -> ë²„ë¦¼
            pass

    # íŒŒì¼ ì“°ê¸°
    for team_key, texts in team_buffers.items():
        if not texts:
            continue
            
        file_path = os.path.join(OUTPUT_DIR, f"{team_key}.txt")
        
        # ê¸°ì¡´ íŒŒì¼ ë‚´ìš© í™•ì¸ (ì¤‘ë³µ ë°©ì§€ìš©)
        existing_content = ""
        file_exists = os.path.exists(file_path)
        
        if file_exists:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    existing_content = f.read()
            except Exception:
                pass # íŒŒì¼ ì½ê¸° ì‹¤íŒ¨ ì‹œ ì¤‘ë³µ ì²´í¬ ê±´ë„ˆëœ€ (ê·¸ëƒ¥ append)

        mode = 'a' if file_exists else 'w'
        
        try:
            with open(file_path, mode, encoding='utf-8') as f:
                # ìƒˆ íŒŒì¼ì´ë©´ í—¤ë” ì‘ì„±
                if not file_exists:
                    f.write(f"íŒ€ ì´ë¦„: {team_key}\n")
                    f.write("========== TEAM NARRATIVE DATA (Namuwiki) ==========\n")
                
                # ë‚´ìš© ì¶”ê°€ (ì¤‘ë³µ ì²´í¬)
                append_count = 0
                for t in texts:
                    # ê¸°ì¡´ íŒŒì¼ì— í…ìŠ¤íŠ¸ê°€ ì—†ê³  (í˜¹ì€ ë„ˆë¬´ ì§§ì•„ êµ¬ë¶„ì´ ì•ˆë˜ê±°ë‚˜), 
                    # í˜„ì¬ ëª¨ìœ¼ê³  ìˆëŠ” existing_contentì—ë„ ì—†ì–´ì•¼ í•¨.
                    # (ë‹¨, existing_contentê°€ ë„ˆë¬´ ì»¤ì§€ë©´ ëŠë ¤ì§ˆ ìˆ˜ ìˆìœ¼ë‚˜ í…ìŠ¤íŠ¸ íŒŒì¼ ìˆ˜ì¤€ì—ì„  OK)
                    if t.strip() not in existing_content:
                        f.write(t + "\n\n")
                        existing_content += t + "\n\n" # ê°™ì€ ì‹¤í–‰ ë£¨í”„ ë‚´ ì¤‘ë³µ ë°©ì§€ ì—…ë°ì´íŠ¸
                        append_count += 1
                
            if append_count > 0:
                print(f"âœ… [{team_key}] {append_count}ê°œ í•­ëª© ì¶”ê°€ ì €ì¥ ì™„ë£Œ: {file_path}")
            else:
                print(f"â„¹ï¸ [{team_key}] ìƒˆë¡œìš´ ë‚´ìš©ì´ ì—†ì–´ ì €ì¥í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ {team_key}: {e}")


async def main_async():
    print("ë‚˜ë¬´ìœ„í‚¤ F1 ë°ì´í„° í¬ë¡¤ë§ ë° ë¶„ë¥˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...\n")
    
    # ì „ì²´ URLì—ì„œ ìˆ˜ì§‘ëœ ëª¨ë“  í…ìŠ¤íŠ¸ (ìˆœì„œ ìœ ì§€)
    all_collected_text = []

    for url in TARGET_URLS:
        data = await crawl_namuwiki_content(url)
        if data:
            all_collected_text.extend(data)
            
    print(f"\nì´ {len(all_collected_text)}ê°œì˜ í…ìŠ¤íŠ¸ ì²­í¬ë¥¼ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤. ë¶„ë¥˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
    
    classify_and_save(all_collected_text)
    
    print("\nëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(main_async())
